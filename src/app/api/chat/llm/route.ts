import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { streamText } from 'ai';
import { NextRequest } from 'next/server';

// Type definition for supported providers
type LLMProvider = 'openai' | 'anthropic' | 'azure-openai' | 'grok' | 'bedrock';

export async function POST(req: NextRequest) {
  console.log('ü§ñ LLM API - Request received');
  
  try {
    const { messages, provider = 'openai', model, temperature = 0.7 } = await req.json();
    console.log('üìù Request payload:', { 
      messageCount: messages?.length,
      provider,
      model,
      temperature,
      messages: messages?.map((m: any) => ({ role: m.role, contentLength: m.content?.length }))
    });

    // Provider configurations
    const getModel = (provider: LLMProvider, model?: string) => {
      console.log(`üîß Configuring model for provider: ${provider}`, { model });
      
      switch (provider) {
        case 'openai':
          console.log('üöÄ Using OpenAI provider');
          return openai(model || 'gpt-3.5-turbo');
        
        case 'anthropic':
          console.log('üß† Using Anthropic provider');
          return anthropic(model || 'claude-3-haiku-20240307');
        
        case 'azure-openai':
          console.log('‚òÅÔ∏è Using Azure OpenAI provider');
          // Azure OpenAI configuration
          return openai(model || 'gpt-3.5-turbo', {
            baseURL: process.env.AZURE_OPENAI_ENDPOINT,
            apiKey: process.env.AZURE_OPENAI_API_KEY,
            apiVersion: '2024-02-01',
          });
        
        case 'grok':
          console.log('‚ö° Using Grok provider');
          // Grok configuration (using OpenAI-compatible interface)
          return openai(model || 'grok-beta', {
            baseURL: 'https://api.x.ai/v1',
            apiKey: process.env.GROK_API_KEY,
          });
        
        case 'bedrock':
          console.log('üèóÔ∏è Using Bedrock provider');
          // Amazon Bedrock (using Anthropic models through Bedrock)
          return anthropic(model || 'claude-3-haiku-20240307', {
            baseURL: `https://bedrock-runtime.${process.env.AWS_REGION || 'us-east-1'}.amazonaws.com`,
            // Bedrock requires special authentication handling
          });
        
        default:
          console.error('‚ùå Unsupported provider:', provider);
          throw new Error(`Unsupported provider: ${provider}`);
      }
    };

    // Validate provider
    console.log('‚úÖ Validating provider...');
    const supportedProviders: LLMProvider[] = ['openai', 'anthropic', 'azure-openai', 'grok', 'bedrock'];
    if (!supportedProviders.includes(provider as LLMProvider)) {
      console.log('‚ùå Invalid provider:', provider);
      return new Response(
        JSON.stringify({ error: `Invalid provider: ${provider}` }),
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      );
    }

    // Check for required API keys
    console.log('üîë Checking API key availability...');
    const apiKeyChecks = {
      openai: process.env.OPENAI_API_KEY,
      anthropic: process.env.ANTHROPIC_API_KEY,
      'azure-openai': process.env.AZURE_OPENAI_API_KEY,
      grok: process.env.GROK_API_KEY,
      bedrock: process.env.AWS_ACCESS_KEY_ID, // For Bedrock
    };

    const hasApiKey = !!apiKeyChecks[provider as keyof typeof apiKeyChecks];
    console.log(`üîê API key status for ${provider}:`, hasApiKey ? 'Available' : 'Missing');
    
    if (!hasApiKey) {
      console.log('‚ùå API key missing for provider:', provider);
      return new Response(
        JSON.stringify({ error: `API key not configured for provider: ${provider}` }),
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      );
    }

    // Stream the chat completion
    console.log('üåä Starting streaming chat completion...');
    const result = await streamText({
      model: getModel(provider as LLMProvider, model),
      messages,
      temperature,
      maxTokens: 1000,
    });
    console.log('‚úÖ Stream created successfully');

    return result.toAIStreamResponse();
  } catch (error) {
    console.error('üí• LLM API error:', {
      name: error.name,
      message: error.message,
      stack: error.stack,
      provider
    });
    return new Response(
      JSON.stringify({ 
        error: 'Internal Server Error',
        details: error instanceof Error ? error.message : 'Unknown error',
        provider
      }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
}
